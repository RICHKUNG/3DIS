{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8bda59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oyea0801/anaconda3/envs/SAM2/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/oyea0801/projects/Semantic-SAM/semantic_sam/body/encoder/encoder_deform.py:355: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled=False)\n",
      "/home/oyea0801/projects/Semantic-SAM/semantic_sam/body/decoder/utils/dino_decoder.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled=False)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from semantic_sam import (\n",
    "    prepare_image,\n",
    "    plot_results,\n",
    "    build_semantic_sam,\n",
    "    SemanticSamAutomaticMaskGenerator,\n",
    ")\n",
    "\n",
    "from sam2_main.sam2.build_sam import build_sam2_video_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bae78a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_regions_with_Semantic_SAM(image_sequence, auto_mask_generator, freq=1):\n",
    "    masks_candidates= []\n",
    "\n",
    "    for t, image in enumerate(image_sequence):\n",
    "            \n",
    "        # generate masks using Semantic-SAM\n",
    "        print(f\"⏳ start auto mask generation using Semantic-SAM on frame {t*freq}\")\n",
    "        current_masks = auto_mask_generator.generate(image)\n",
    "        print(f\"✅ auto mask generation finish, {len(current_masks)} masks generated\")\n",
    "\n",
    "        # add frame index to each mask for further SAM2 tracking\n",
    "        for mask in current_masks:\n",
    "            mask[\"frame\"] = t*10\n",
    "        \n",
    "        masks_candidates.append(current_masks)\n",
    "\n",
    "    # tracked_masks = [mask for frame_masks in tracked_masks for mask in frame_masks]  # flatten the list\n",
    "    print(\"✅ Semantic-SAM processed all frames\")\n",
    "    \n",
    "    return masks_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11defbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool functions\n",
    "def compute_iou(mask1, mask2):\n",
    "    # If masks are already the same size, compute IoU directly\n",
    "    if mask1.shape == mask2.shape:\n",
    "        intersection = np.logical_and(mask1, mask2).sum()\n",
    "        union = np.logical_or(mask1, mask2).sum()\n",
    "        return intersection / union if union != 0 else 0\n",
    "\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    m1 = torch.from_numpy(mask1.astype(np.float32))\n",
    "    m2 = torch.from_numpy(mask2.astype(np.float32))\n",
    "\n",
    "    # Add batch and channel dimensions (B, C, H, W)\n",
    "    m1 = m1.unsqueeze(0).unsqueeze(0)\n",
    "    m2 = m2.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Resize m2 to match m1's dimensions using bilinear interpolation\n",
    "    m2_resized = torch.nn.functional.interpolate(m2, size=m1.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "    # Threshold the resized mask to make it boolean again and remove batch/channel dims\n",
    "    m2_resized = (m2_resized > 0.5).squeeze()\n",
    "    m1 = m1.squeeze() # Remove batch/channel dims\n",
    "\n",
    "    # Now that they are the same size, compute IoU\n",
    "    intersection = torch.logical_and(m1, m2_resized).sum().item()\n",
    "    union = torch.logical_or(m1, m2_resized).sum().item()\n",
    "\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "# loading and selecting scene color images into one list\n",
    "def load_scene_img(datapath, freq=1):\n",
    "    \n",
    "    # scene sampling parameters\n",
    "    start_idx = 0\n",
    "    end_idx = 200\n",
    "    selected_img = []\n",
    "\n",
    "    # select the wanted image sequence \n",
    "    color_img = sorted(os.listdir(datapath))\n",
    "    selected_img_idx = [color_img[i] for i in range(start_idx, end_idx, freq)]    \n",
    "    print(f\"Selected image number: {len(selected_img_idx)}\")\n",
    "    print(\"selected frame:\", selected_img_idx) \n",
    "\n",
    "    # pre-processing images\n",
    "    for img_name in selected_img_idx:\n",
    "        image_path = os.path.join(datapath, img_name)\n",
    "        _, image = prepare_image(image_path)\n",
    "        selected_img.append(image)\n",
    "\n",
    "    return selected_img\n",
    "\n",
    "def bbox_transform(bboxes):\n",
    "    for bbox in bboxes:\n",
    "        x, y, w, h = bbox\n",
    "        bbox[0] = x\n",
    "        bbox[1] = y\n",
    "        bbox[2] = x + w\n",
    "        bbox[3] = y + h\n",
    "    return bboxes\n",
    "\n",
    "def bbox_scalar_fit(bboxes, scalar_x, scalar_y):\n",
    "    for bbox in bboxes:\n",
    "        bbox[0] = int(bbox[0] * scalar_x)\n",
    "        bbox[1] = int(bbox[1] * scalar_y)\n",
    "        bbox[2] = int(bbox[2] * scalar_x)\n",
    "        bbox[3] = int(bbox[3] * scalar_y)\n",
    "    return bboxes\n",
    "\n",
    "def mask_candidate_refine(mask_candidates, min_area=300, max_area=50000, stability_score=0.9):\n",
    "    final_candidates = []\n",
    "    filtered_out = 0\n",
    "\n",
    "    for frame in mask_candidates:\n",
    "        refined_candidates = []\n",
    "        for mask in frame:\n",
    "            area = mask['area']\n",
    "            if min_area <= area <= max_area:\n",
    "                if mask['stability_score'] >= stability_score:\n",
    "                    refined_candidates.append(mask)\n",
    "            else:\n",
    "                filtered_out += 1\n",
    "        else:\n",
    "            filtered_out += 1\n",
    "        \n",
    "        final_candidates.append(refined_candidates)\n",
    "\n",
    "    print(f\"Filtered out {filtered_out} masks based on area and stability score.\")\n",
    "\n",
    "    return final_candidates\n",
    "\n",
    "def store_output(output_path, data_path, video_segments):\n",
    "    \"\"\"\n",
    "    Stores the masked frames for each object in separate folders.\n",
    "\n",
    "    Args:\n",
    "        output_path (str): The root directory to save the output folders.\n",
    "        data_path (str): The path to the directory of original video frames.\n",
    "        video_segments (dict): A dictionary containing the segmentation masks.\n",
    "                               It is structured as {frame_idx: {obj_id: mask_array, ...}}.\n",
    "    \"\"\"\n",
    "    print(f\"⏳ Storing output masks to {output_path}\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    # Find all unique object IDs from the tracking results\n",
    "    all_obj_ids = set()\n",
    "    for frame_masks in video_segments.values():\n",
    "        all_obj_ids.update(frame_masks.keys())\n",
    "\n",
    "    # Create a folder for each object ID\n",
    "    for obj_id in all_obj_ids:\n",
    "        obj_folder = os.path.join(output_path, str(obj_id))\n",
    "        os.makedirs(obj_folder, exist_ok=True)\n",
    "    \n",
    "    print(f\"Found {len(all_obj_ids)} unique objects to store.\")\n",
    "\n",
    "    # Get a sorted list of the original frame filenames\n",
    "    try:\n",
    "        frame_filenames = sorted([f for f in os.listdir(data_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The data_path '{data_path}' was not found.\")\n",
    "        return\n",
    "\n",
    "    # Process each frame\n",
    "    for frame_idx, frame_filename in enumerate(tqdm(frame_filenames, desc=\"Applying masks and saving frames\")):\n",
    "        if frame_idx in video_segments:\n",
    "            # Load the original image\n",
    "            original_img_path = os.path.join(data_path, frame_filename)\n",
    "            original_img = Image.open(original_img_path).convert('RGB')\n",
    "            original_img_array = np.array(original_img)\n",
    "\n",
    "            # Get the masks for the current frame\n",
    "            frame_masks = video_segments[frame_idx]\n",
    "            for obj_id, mask in frame_masks.items():\n",
    "                # Ensure the mask is boolean\n",
    "                bool_mask = mask > 0\n",
    "                bool_mask = np.squeeze(bool_mask)\n",
    "                \n",
    "                # Apply the mask to the original image using broadcasting\n",
    "                # np.newaxis adds a new dimension to the mask (H, W) -> (H, W, 1)\n",
    "                # so it can be multiplied with the image array (H, W, 3)\n",
    "                masked_img_array = original_img_array * bool_mask[:, :, np.newaxis]\n",
    "\n",
    "                # Convert array back to image and save\n",
    "                masked_img = Image.fromarray(masked_img_array.astype(np.uint8))\n",
    "                save_filename = os.path.splitext(frame_filename)[0] + \".png\" # Save as png\n",
    "                save_path = os.path.join(output_path, str(obj_id), save_filename)\n",
    "                masked_img.save(save_path)\n",
    "\n",
    "    print(f\"✅ Storing output finished successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dff863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAM2_tracking(data_path, SAM2_video_predictor, mask_candidates, freq=1):\n",
    "    with torch.inference_mode(), torch.autocast(\"cuda\"):\n",
    "        iou_threshold = 0.6\n",
    "        obj_count = 1  # to keep track of the total number of objects added\n",
    "        final_video_segments = {}\n",
    "\n",
    "        # Initialize SAM2 video predictor state\n",
    "        inference_state = SAM2_video_predictor.init_state(video_path=data_path)\n",
    "\n",
    "        # Calculate scaling factors based on the first frame's mask size\n",
    "        scalar_x = inference_state['video_width'] / mask_candidates[0][0]['segmentation'].shape[1]\n",
    "        scalar_y = inference_state['video_height'] / mask_candidates[0][0]['segmentation'].shape[0]\n",
    "        \n",
    "        for frame, frame_masks in enumerate(mask_candidates):\n",
    "            if frame == 3:\n",
    "                break  # demo only\n",
    "            # Re-initialize the state to process only the new masks for this iteration\n",
    "            inference_state = SAM2_video_predictor.init_state(video_path=data_path)\n",
    "            \n",
    "            if frame > 0:\n",
    "                untracked_masks = []\n",
    "                for mask in frame_masks:\n",
    "                    is_tracked = False\n",
    "                    if any(compute_iou(prev_mask, mask[\"segmentation\"]) > iou_threshold for prev_mask in final_video_segments[frame*freq].values()):\n",
    "                        is_tracked = True\n",
    "                    if not is_tracked:\n",
    "                        untracked_masks.append(mask)\n",
    "\n",
    "                print(f\"{len(untracked_masks)} untracked masks found on frame: {frame*freq}.\")\n",
    "                bboxes = [mask['bbox'] for mask in untracked_masks]\n",
    "            else:\n",
    "                bboxes = [mask['bbox'] for mask in frame_masks]\n",
    "                \n",
    "            # Convert Semantic-SAM bbox format (XYWH) to SAM2 bbox format (x1y1x2y2)\n",
    "            bboxes = bbox_transform(bboxes)\n",
    "                \n",
    "            # Scale the bboxes to fit the SAM2 video input image size\n",
    "            bboxes = bbox_scalar_fit(bboxes, scalar_x, scalar_y)\n",
    "                \n",
    "            # Apply SAM2 to generate masklets across frames\n",
    "            for bbox in bboxes:\n",
    "                if obj_count % 3 == 0:\n",
    "                    obj_count += 1\n",
    "                    break  # demo onlyirst 3 mask candidates for tracking\n",
    "\n",
    "                ann_frame_idx = frame*freq\n",
    "                ann_obj_id = obj_count\n",
    "                obj_count += 1\n",
    "\n",
    "                print(f\"⏳ start adding masklet {obj_count-1} from frame {ann_frame_idx} as prompt for SAM2\")\n",
    "                _, out_obj_ids, out_mask_logits = SAM2_video_predictor.add_new_points_or_box(inference_state=inference_state, frame_idx=ann_frame_idx, obj_id=ann_obj_id, box=bbox)\n",
    "\n",
    "            # Propagate the masks through the video\n",
    "            video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "            for out_frame_idx, out_obj_ids, out_mask_logits in SAM2_video_predictor.propagate_in_video(inference_state):\n",
    "                video_segments[out_frame_idx] = {\n",
    "                out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "                for i, out_obj_id in enumerate(out_obj_ids)\n",
    "            }\n",
    "            \n",
    "            \n",
    "            for out_frame_idx, frame_data in video_segments.items():\n",
    "                if out_frame_idx not in final_video_segments:\n",
    "                    final_video_segments[out_frame_idx] = {}\n",
    "                final_video_segments[out_frame_idx].update(frame_data)\n",
    "        \n",
    "        print(\"✅ Iterative video propagation finished.\")\n",
    "        return final_video_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5148e96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"Multiscan/scene0065_00/color/JPG\"\n",
    "freq = 10 # image sampling frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079a5cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ start loading scene images\n",
      "Selected image number: 20\n",
      "selected frame: ['1400.jpg', '1410.jpg', '1420.jpg', '1430.jpg', '1440.jpg', '1450.jpg', '1460.jpg', '1470.jpg', '1480.jpg', '1490.jpg', '1500.jpg', '1510.jpg', '1520.jpg', '1530.jpg', '1540.jpg', '1551.jpg', '1561.jpg', '1571.jpg', '1581.jpg', '1591.jpg']\n",
      "✅ loading scene image finish\n"
     ]
    }
   ],
   "source": [
    "# loading scene color image \n",
    "print(\"⏳ start loading scene images\")\n",
    "image_sequence= load_scene_img(data_path, freq=10)\n",
    "print(\"✅ loading scene image finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00328548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ start loading Semantic-SAM model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oyea0801/anaconda3/envs/SAM2/lib/python3.11/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4322.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "$UNUSED$ criterion.empty_weight, Ckpt Shape: torch.Size([2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Semantic-SAM model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# loading Semantic-SAM model\n",
    "print(\"⏳ start loading Semantic-SAM model\")\n",
    "model_type = 'L'\n",
    "ckpt_path = \"ckpts/swinl_only_sam_many2many.pth\"\n",
    "auto_mask_generator = SemanticSamAutomaticMaskGenerator(build_semantic_sam(model_type=model_type, ckpt=ckpt_path))\n",
    "print(\"✅ Semantic-SAM model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49067d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ start loading SAM2 model\n",
      "✅ SAM2 model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# loading SAM2 video predictor\n",
    "print(\"⏳ start loading SAM2 model\")\n",
    "checkpoint = \"sam2_main/checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "SAM2_video_predictor = build_sam2_video_predictor(model_cfg, checkpoint)\n",
    "print(\"✅ SAM2 model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8282593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ start auto mask generation using Semantic-SAM on frame 0\n",
      "✅ auto mask generation finish, 83 masks generated\n",
      "⏳ start auto mask generation using Semantic-SAM on frame 10\n",
      "✅ auto mask generation finish, 74 masks generated\n",
      "⏳ start auto mask generation using Semantic-SAM on frame 20\n",
      "✅ auto mask generation finish, 75 masks generated\n",
      "⏳ start auto mask generation using Semantic-SAM on frame 30\n",
      "✅ auto mask generation finish, 73 masks generated\n",
      "⏳ start auto mask generation using Semantic-SAM on frame 40\n",
      "✅ auto mask generation finish, 62 masks generated\n",
      "⏳ start auto mask generation using Semantic-SAM on frame 50\n",
      "✅ auto mask generation finish, 72 masks generated\n",
      "⏳ start auto mask generation using Semantic-SAM on frame 60\n",
      "✅ auto mask generation finish, 61 masks generated\n",
      "⏳ start auto mask generation using Semantic-SAM on frame 70\n",
      "✅ auto mask generation finish, 75 masks generated\n",
      "⏳ start auto mask generation using Semantic-SAM on frame 80\n",
      "✅ auto mask generation finish, 75 masks generated\n",
      "⏳ start auto mask generation using Semantic-SAM on frame 90\n",
      "✅ auto mask generation finish, 72 masks generated\n",
      "⏳ start auto mask generation using Semantic-SAM on frame 100\n",
      "✅ auto mask generation finish, 76 masks generated\n",
      "⏳ start auto mask generation using Semantic-SAM on frame 110\n",
      "✅ auto mask generation finish, 84 masks generated\n",
      "⏳ start auto mask generation using Semantic-SAM on frame 120\n",
      "✅ auto mask generation finish, 78 masks generated\n",
      "⏳ start auto mask generation using Semantic-SAM on frame 130\n",
      "✅ auto mask generation finish, 79 masks generated\n",
      "⏳ start auto mask generation using Semantic-SAM on frame 140\n",
      "✅ auto mask generation finish, 81 masks generated\n",
      "⏳ start auto mask generation using Semantic-SAM on frame 150\n",
      "✅ auto mask generation finish, 65 masks generated\n",
      "⏳ start auto mask generation using Semantic-SAM on frame 160\n",
      "✅ auto mask generation finish, 71 masks generated\n",
      "⏳ start auto mask generation using Semantic-SAM on frame 170\n",
      "✅ auto mask generation finish, 76 masks generated\n",
      "⏳ start auto mask generation using Semantic-SAM on frame 180\n",
      "✅ auto mask generation finish, 63 masks generated\n",
      "⏳ start auto mask generation using Semantic-SAM on frame 190\n",
      "✅ auto mask generation finish, 71 masks generated\n",
      "✅ Semantic-SAM processed all frames\n",
      "Filtered out 217 masks based on area and stability score.\n"
     ]
    }
   ],
   "source": [
    "# implement algorithm 1's first-half\n",
    "# generate mask candidates using Semantic-SAM\n",
    "tracked_masks = extract_regions_with_Semantic_SAM(image_sequence, auto_mask_generator, freq)\n",
    "tracked_masks = mask_candidate_refine(tracked_masks) # refine mask candidates based on area and stability score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f23ea958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 200/200 [00:06<00:00, 31.65it/s]\n",
      "frame loading (JPEG): 100%|██████████| 200/200 [00:05<00:00, 34.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ start adding masklet 1 from frame 0 as prompt for SAM2\n",
      "⏳ start adding masklet 2 from frame 0 as prompt for SAM2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 200/200 [03:35<00:00,  1.08s/it]\n",
      "frame loading (JPEG): 100%|██████████| 200/200 [00:09<00:00, 22.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 untracked masks found on frame: 10.\n",
      "⏳ start adding masklet 4 from frame 10 as prompt for SAM2\n",
      "⏳ start adding masklet 5 from frame 10 as prompt for SAM2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 190/190 [00:28<00:00,  6.77it/s]\n",
      "frame loading (JPEG): 100%|██████████| 200/200 [00:06<00:00, 31.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 untracked masks found on frame: 20.\n",
      "⏳ start adding masklet 7 from frame 20 as prompt for SAM2\n",
      "⏳ start adding masklet 8 from frame 20 as prompt for SAM2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 180/180 [01:47<00:00,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Iterative video propagation finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "video_segments = SAM2_tracking(data_path, SAM2_video_predictor, tracked_masks, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58e1da99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Storing output masks to output/test_scene_0065_00\n",
      "Found 6 unique objects to store.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying masks and saving frames: 100%|██████████| 200/200 [00:51<00:00,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Storing output finished successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# store the video_segments results\n",
    "output_path = \"output/test_scene_0065_00\"\n",
    "store_output(output_path, data_path, video_segments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAM2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
